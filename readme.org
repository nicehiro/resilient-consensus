* Opinions

** RL based
*** DQN
将 DQN 的方法直接运用到节点收敛问题上，目前两种方法实现：
- FAILED 没有经验池的概念，对每一步都进行网络更新
- 使用经验池，使用大量数据训练网络，直到网络训练的差不多，使用训练好的网络进行测试

*** PG
将 PG 的方法直接运用到节点收敛问题上，目前两种方法实现：
- FAILED 没有经验池的概念，对每一步都进行网络更新
- 使用经验池，使用大量数据训练网络，直到网络训练的差不多，使用训练好的网络进行测试

*** DDPG
将 DDPG 的方法直接运用到节点收敛问题上，目前两种方法实现：
- FAILED 没有经验池的概念，对每一步都进行网络更新
- 使用经验池，使用大量数据训练网络，直到网络训练的差不多，使用训练好的网络进行测试

*** NEW MADDPG
- 由于当前环境中每个智能体都可以进行决策，破坏了马尔可夫性质，所以当前的学习方法不适合，应该使用 MADDPG 尝试一下。

** Q_New
提出的新算法的实现。

** Battle
